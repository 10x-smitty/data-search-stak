input {
  file {
    # Read CSV files from the ingest_data directory
    mode => "read"
    path => "/usr/share/logstash/ingest_data/*.csv"
    exit_after_read => true
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/ingest_data/logstash_completed.log"
  }
}

filter {
  # Parse CSV data with automatic column detection - EACH ROW BECOMES A SEPARATE DOCUMENT
  csv {
    separator => ","
    autodetect_column_names => true
    autogenerate_column_names => true
  }

  # Skip empty rows
  if "_csvskipped" in [tags] {
    drop { }
  }

  # Enhanced processing for data reconciliation
  ruby {
    code => '
      # Get all field names except metadata fields
      searchable_fields = event.to_hash.keys.reject { |k| 
        k.start_with?("@") || 
        ["host", "path", "tags", "message", "vector_strategy", "embed_field"].include?(k) 
      }
      
      # Create multiple searchable content variations for better matching
      content_parts = []
      normalized_parts = []
      key_identifier_parts = []
      
      searchable_fields.each do |field|
        value = event.get(field)
        if value && !value.to_s.strip.empty?
          original_value = value.to_s.strip
          
          # Original formatted content
          field_name = field.gsub("_", " ").gsub(/\b\w/) { |match| match.upcase }
          content_parts << "#{field_name}: #{original_value}"
          
          # Normalized content (for better fuzzy matching)
          normalized_value = original_value
            .downcase
            .gsub(/[^\w\s]/, " ")  # Remove punctuation
            .gsub(/\s+/, " ")      # Normalize spaces
            .strip
          normalized_parts << "#{field.downcase}: #{normalized_value}"
          
          # Extract key identifiers (numbers, codes, IDs)
          if original_value.match(/\b\d+\b/) || field.downcase.match(/id|code|number|ipi/)
            key_identifier_parts << "#{field.downcase}: #{original_value}"
          end
        end
      end
      
      # Set the main searchable content (original format)
      event.set("searchable_content", content_parts.join(". "))
      
      # Set normalized content for fuzzy matching
      event.set("normalized_content", normalized_parts.join(". "))
      
      # Set key identifiers content (for exact ID matches)
      event.set("key_identifiers", key_identifier_parts.join(". "))
      
      # Store metadata about this row
      event.set("csv_fields", searchable_fields)
      event.set("field_count", searchable_fields.length)
      event.set("has_key_identifiers", !key_identifier_parts.empty?)
    '
  }

  # Skip rows with no meaningful content
  if [searchable_content] == "" {
    drop { }
  }

  # Enhanced data type detection and normalization
  ruby {
    code => '
      event.to_hash.each do |field, value|
        next if field.start_with?("@") || ["host", "path", "tags", "message", "searchable_content", "normalized_content", "key_identifiers", "csv_fields", "field_count", "vector_strategy", "embed_field"].include?(field)
        
        if value.is_a?(String) && !value.strip.empty?
          original_value = value.strip
          
          # Try to extract and normalize person names
          if field.downcase.match(/name|writer|composer|artist|author/)
            # Normalize name variations (Smith, John -> John Smith)
            if original_value.match(/^([^,]+),\s*(.+)$/)
              normalized_name = "#{$2.strip} #{$1.strip}"
              event.set("#{field}_normalized", normalized_name)
            end
          end
          
          # Try to convert to integer
          if original_value.match(/^\d+$/)
            event.set(field, original_value.to_i)
          # Try to convert to float  
          elsif original_value.match(/^\d*\.\d+$/)
            event.set(field, original_value.to_f)
          # Extract IPI numbers or similar codes
          elsif original_value.match(/\b\d{8,}\b/)
            codes = original_value.scan(/\b\d{8,}\b/)
            event.set("#{field}_codes", codes) if codes.any?
          # Try to parse common date formats
          elsif original_value.match(/^\d{1,2}\/\d{1,2}\/\d{4}$/) || original_value.match(/^\d{4}-\d{2}-\d{2}$/)
            begin
              if original_value.match(/^\d{1,2}\/\d{1,2}\/\d{4}$/)
                parsed_date = Date.strptime(original_value, "%m/%d/%Y")
              else
                parsed_date = Date.strptime(original_value, "%Y-%m-%d")
              end
              event.set("#{field}_parsed", parsed_date.strftime("%Y-%m-%dT%H:%M:%S.%LZ"))
            rescue
              # Keep original value if parsing fails
            end
          end
        end
      end
    '
  }

  # Add source file information for traceability
  mutate {
    add_field => {
      "source_file" => "%{[path]}"
    }
  }

  # Extract filename from path
  grok {
    match => { "path" => ".*/(?<filename>[^/]+\.csv)$" }
    tag_on_failure => ["_grokparsefailure_filename"]
  }

  # Create a source identifier (useful for tracking which PRO/source the data came from)
  mutate {
    add_field => {
      "data_source" => "%{[filename]}"
    }
  }

  # Clean up all searchable content fields
  mutate {
    gsub => [
      "searchable_content", "\s+", " ",
      "searchable_content", "^\s+|\s+$", "",
      "normalized_content", "\s+", " ",
      "normalized_content", "^\s+|\s+$", "",
      "key_identifiers", "\s+", " ",
      "key_identifiers", "^\s+|\s+$", ""
    ]
  }

  # Add metadata for vector processing with multiple embedding strategies
  mutate {
    add_field => {
      "vector_strategy" => "huggingface"
      "embed_field" => "searchable_content"
      "document_type" => "csv_row_reconciliation"
      "reconciliation_ready" => "true"
    }
  }
}

output {
  elasticsearch {
    # Use source-aware indexing for reconciliation
    index => "reconciliation-%{[data_source]}-%{+YYYY.MM}"
    hosts => "${ELASTIC_HOSTS}"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    cacert => "certs/ca/ca.crt"
    
    # Use the ingest pipeline for vector generation
    pipeline => "vector-embeddings-pipeline"
    
    # Generate document ID for uniqueness and deduplication detection
    document_id => "%{[data_source]}-%{[@metadata][file_position]}"
  }

  # Optional: Output to stdout for debugging
  # stdout {
  #   codec => rubydebug
  # }
}
