input {
  file {
    # Read CSV files from the ingest_data directory
    mode => "read"
    path => "/usr/share/logstash/ingest_data/*.csv"
    exit_after_read => true
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/ingest_data/logstash_completed.log"
  }
}

filter {
  # Parse CSV data with automatic column detection - EACH ROW BECOMES A SEPARATE DOCUMENT
  csv {
    separator => ","
    autodetect_column_names => true
    autogenerate_column_names => true
  }

  # Skip empty rows
  if "_csvskipped" in [tags] {
    drop { }
  }

  # Create searchable content by combining ALL fields from this row
  ruby {
    code => '
      # Get all field names except metadata fields
      searchable_fields = event.to_hash.keys.reject { |k| 
        k.start_with?("@") || 
        ["host", "path", "tags", "message", "vector_strategy", "embed_field"].include?(k) 
      }
      
      # Combine all field values from this CSV row into searchable content
      content_parts = []
      searchable_fields.each do |field|
        value = event.get(field)
        if value && !value.to_s.strip.empty?
          # Format as "field_name: value" for better embedding context
          field_name = field.gsub("_", " ").gsub(/\b\w/) { |match| match.upcase }
          content_parts << "#{field_name}: #{value}"
        end
      end
      
      # Set the searchable content for this individual row/record
      event.set("searchable_content", content_parts.join(". "))
      
      # Store metadata about this row
      event.set("csv_fields", searchable_fields)
      event.set("field_count", searchable_fields.length)
    '
  }

  # Skip rows with no meaningful content
  if [searchable_content] == "" {
    drop { }
  }

  # Auto-detect and convert numeric fields for better indexing
  ruby {
    code => '
      event.to_hash.each do |field, value|
        next if field.start_with?("@") || ["host", "path", "tags", "message", "searchable_content", "csv_fields", "field_count", "vector_strategy", "embed_field"].include?(field)
        
        if value.is_a?(String) && !value.strip.empty?
          # Try to convert to integer
          if value.match(/^-?\d+$/)
            event.set(field, value.to_i)
          # Try to convert to float  
          elsif value.match(/^-?\d*\.\d+$/)
            event.set(field, value.to_f)
          # Try to parse common date formats
          elsif value.match(/^\d{1,2}\/\d{1,2}\/\d{4}$/) || value.match(/^\d{4}-\d{2}-\d{2}$/)
            begin
              if value.match(/^\d{1,2}\/\d{1,2}\/\d{4}$/)
                parsed_date = Date.strptime(value, "%m/%d/%Y")
              else
                parsed_date = Date.strptime(value, "%Y-%m-%d")
              end
              event.set("#{field}_parsed", parsed_date.strftime("%Y-%m-%dT%H:%M:%S.%LZ"))
            rescue
              # Keep original value if parsing fails
            end
          end
        end
      end
    '
  }

  # Add source file information for traceability
  mutate {
    add_field => {
      "source_file" => "%{[path]}"
    }
  }

  # Extract filename from path
  grok {
    match => { "path" => ".*/(?<filename>[^/]+\.csv)$" }
    tag_on_failure => ["_grokparsefailure_filename"]
  }

  # Clean up the searchable content
  mutate {
    gsub => [
      "searchable_content", "\s+", " ",
      "searchable_content", "^\s+|\s+$", ""
    ]
  }

  # Add metadata for vector processing
  mutate {
    add_field => {
      "vector_strategy" => "huggingface"
      "embed_field" => "searchable_content"
      "document_type" => "csv_row"
    }
  }
}

output {
  elasticsearch {
    # Generic index pattern for CSV data with vectors (uses filename for organization)
    index => "csv-vector-%{[filename]}-%{+YYYY.MM}"
    hosts => "${ELASTIC_HOSTS}"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    cacert => "certs/ca/ca.crt"
    
    # Use the ingest pipeline for vector generation
    pipeline => "vector-embeddings-pipeline"
    
    # Generate document ID from file path and line number for uniqueness
    document_id => "%{[host]}-%{[path]}-%{[@metadata][file_position]}"
  }

  # Optional: Output to stdout for debugging (remove in production)
  # stdout {
  #   codec => rubydebug
  # }
}
