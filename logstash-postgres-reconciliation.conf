input {
  # PostgreSQL JDBC Input with Enhanced Reconciliation Support
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/postgresql-42.7.1.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/${POSTGRES_DB}?currentSchema=public"
    jdbc_user => "${POSTGRES_USER}"
    jdbc_password => "${POSTGRES_PASSWORD}"
    
    # Schedule - run every 2 minutes for reconciliation updates
    schedule => "*/2 * * * *"
    
    # Enhanced query with music rights data for comprehensive matching
    statement => "
      SELECT 
        sc.id,
        sc.title,
        sc.alternate_titles,
        sc.iswc,
        sc.genre,
        sc.writers,
        sc.writer_pros,
        sc.publishers,
        sc.total_performance_revenue,
        sc.total_mechanical_revenue,
        'songs_complete' as table_name,
        'postgresql_music_rights' as data_source_table
      FROM songs_complete sc 
      ORDER BY sc.id
    "
    
    # Add metadata for reconciliation indexing
    add_field => { 
      "data_source" => "postgresql"
      "table_name" => "songs_complete"
      "vector_strategy" => "huggingface"
      "embed_field" => "searchable_content"
      "document_type" => "postgres_row_reconciliation"
      "reconciliation_ready" => "true"
    }
    
    # Tag for identification
    tags => ["postgresql", "music_rights", "vector_enabled", "reconciliation"]
  }
}

filter {
  # Enhanced processing for PostgreSQL data reconciliation
  ruby {
    code => '
      # Get all field names except metadata fields
      searchable_fields = event.to_hash.keys.reject { |k| 
        k.start_with?("@") || 
        ["host", "path", "tags", "message", "vector_strategy", "embed_field", "data_source", "table_name", "document_type"].include?(k) 
      }
      
      # Create multiple searchable content variations for better matching
      content_parts = []
      normalized_parts = []
      key_identifier_parts = []
      
      searchable_fields.each do |field|
        value = event.get(field)
        if value && !value.to_s.strip.empty?
          original_value = value.to_s.strip
          
          # Original formatted content
          field_name = field.gsub("_", " ").gsub(/\b\w/) { |match| match.upcase }
          content_parts << "#{field_name}: #{original_value}"
          
          # Normalized content (for better fuzzy matching)
          normalized_value = original_value
            .downcase
            .gsub(/[^\w\s@.]/, " ")  # Keep @ for emails, . for domains
            .gsub(/\s+/, " ")       # Normalize spaces
            .strip
          normalized_parts << "#{field.downcase}: #{normalized_value}"
          
          # Extract key identifiers (numbers, emails, IDs)
          if original_value.match(/\b\d+\b/) || field.downcase.match(/id|email|phone|number/) || original_value.match(/@/)
            key_identifier_parts << "#{field.downcase}: #{original_value}"
          end
        end
      end
      
      # Set the main searchable content (original format)
      event.set("searchable_content", content_parts.join(". "))
      
      # Set normalized content for fuzzy matching
      event.set("normalized_content", normalized_parts.join(". "))
      
      # Set key identifiers content (for exact ID/email matches)
      event.set("key_identifiers", key_identifier_parts.join(". "))
      
      # Store metadata about this row
      event.set("postgres_fields", searchable_fields)
      event.set("field_count", searchable_fields.length)
      event.set("has_key_identifiers", !key_identifier_parts.empty?)
    '
  }

  # Skip rows with no meaningful content
  if [searchable_content] == "" {
    drop { }
  }

  # Enhanced data type detection and normalization for PostgreSQL data
  ruby {
    code => '
      event.to_hash.each do |field, value|
        next if field.start_with?("@") || ["host", "path", "tags", "message", "searchable_content", "normalized_content", "key_identifiers", "postgres_fields", "field_count", "vector_strategy", "embed_field"].include?(field)
        
        if value && !value.to_s.strip.empty?
          original_value = value.to_s.strip
          
          # Enhanced name processing for person names
          if field.downcase.match(/name|writer|composer|artist|author|contact/)
            # Normalize name variations (Smith, John -> John Smith)
            if original_value.match(/^([^,]+),\s*(.+)$/)
              normalized_name = "#{$2.strip} #{$1.strip}"
              event.set(field + "_normalized", normalized_name)
            end
            # Extract first and last names separately
            name_parts = original_value.split(/\s+/)
            if name_parts.length >= 2
              event.set(field + "_first", name_parts[0])
              event.set(field + "_last", name_parts[-1])
            end
          end
          
          # Enhanced email processing
          if field.downcase.match(/email/) && original_value.match(/@/)
            # Extract domain for matching
            domain = original_value.split("@")[1]
            event.set(field + "_domain", domain) if domain
            # Normalize email (lowercase)
            event.set(field + "_normalized", original_value.downcase)
          end
          
          # Phone number processing
          if field.downcase.match(/phone/)
            # Extract digits only for matching
            digits_only = original_value.gsub(/\D/, "")
            event.set(field + "_digits", digits_only) if digits_only.length >= 10
          end
          
          # Extract any numeric IDs or codes
          if original_value.match(/\b\d{3,}\b/)
            codes = original_value.scan(/\b\d{3,}\b/)
            event.set(field + "_codes", codes) if codes.any?
          end
        end
      end
    '
  }

  # Convert PostgreSQL timestamp
  if [created_at] {
    date {
      match => [ "created_at", "yyyy-MM-dd HH:mm:ss.SSSSSS", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
      target => "created_timestamp"
    }
  }

  # Add source identification for cross-source reconciliation
  mutate {
    add_field => {
      "source_system" => "postgresql"
      "source_table" => "%{table_name}"
      "record_id" => "%{id}"
    }
  }

  # Clean up all searchable content fields
  mutate {
    gsub => [
      "searchable_content", "\s+", " ",
      "searchable_content", "^\s+|\s+$", "",
      "normalized_content", "\s+", " ", 
      "normalized_content", "^\s+|\s+$", "",
      "key_identifiers", "\s+", " ",
      "key_identifiers", "^\s+|\s+$", ""
    ]
  }

  # Add timestamp for Elasticsearch
  mutate {
    add_field => { 
      "ingestion_timestamp" => "%{@timestamp}",
      "environment" => "development"
    }
  }
}

output {
  elasticsearch {
    # Use source-aware indexing for reconciliation (matches CSV pattern)
    index => "reconciliation-postgresql-music-rights-%{+YYYY.MM}"
    hosts => ["https://es01:9200"]
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    cacert => "certs/ca/ca.crt"
    
    # Use the same ingest pipeline as CSV for consistent vector generation
    pipeline => "vector-embeddings-pipeline"
    
    # Generate document ID for uniqueness and deduplication detection
    document_id => "postgres-music-rights-%{id}"
  }

  # Optional: Output to stdout for debugging
  # stdout {
  #   codec => line {
  #     format => "POSTGRES RECONCILIATION: %{name} (%{email}) - Content: %{searchable_content}"
  #   }
  # }
}
